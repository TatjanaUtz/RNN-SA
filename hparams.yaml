default: &DEFAULT
  # TRAINING
  batch_size: 32   # size of each batch of data that is feed into the model
  num_epochs: 10 # number of iterations to run the dataset through the model
  verbose_training: 2  # Integer, 0, 1, or 2; verbosity mode, 0 = silent, 1 = progress bar,
  # 2 = one line per epoch (default: 1)
  validation_split: 0.25  # fraction of the training data to be used as validation data

  # EVALUATION
  verbose_eval: 1 # 0 or 1, verbosity mode, 0 = silent, 1 = progress bar

  # OPTIMIZER
  optimizer: 'adam' # String (name of optimizer) or optimizer instance
  learning_rate: 0.001 # float >= 0, learning rate (default: depends on optimizer, see
  # https://keras.io/optimizers/)

  # DATA
  time_steps: 4 # number of time steps = sequence length (= maximal number of task per task-set)
  element_size: 9 # length of each vector in the sequence (= number of attributes per task)
  num_classes: 1  # number of classes, binary classification, labels are in one column
  train_size: 0.8 # fraction of the data to be used as training dataset
  test_size: 0.2  # fraction of the data to be used as test dataset

SimpleRNN:
  <<: *DEFAULT
  hidden_layer_size: 27 # size of RNN hidden dimension, 3 times the amount of element_size
  hidden_activation_function: 'tanh'  # activation function to use; if you pass None, no activation is
  # applied (ie. "linear" activation: a(x) = x) (default: 'tanh')
  keep_prob: 1.0  # float between 0 and 1, fraction of the input units to drop
  num_cells: 1  # number of LSTM cells

GRU:
  <<: *DEFAULT
  hidden_layer_size: 27 # size of RNN hidden dimension, 3 times the amount of element_size
  hidden_activation_function: 'tanh'  # activation function to use; if you pass None, no activation is
  # applied (ie. "linear" activation: a(x) = x) (default: 'tanh')
  keep_prob: 1.0  # float between 0 and 1, fraction of the input units to drop
  num_cells: 1  # number of LSTM cells

LSTM:
  <<: *DEFAULT
  hidden_layer_size: 27 # size of RNN hidden dimension, 3 times the amount of element_size
  hidden_activation_function: 'tanh'  # activation function to use; if you pass None, no activation is
  # applied (ie. "linear" activation: a(x) = x) (default: 'tanh')
  keep_prob: 1.0  # float between 0 and 1, fraction of the input units to drop
  num_cells: 1  # number of LSTM cells
